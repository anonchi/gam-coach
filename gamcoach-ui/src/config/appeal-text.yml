# Text for the article

summary:
  intro: >
    We appreciate all reviewers’ insightful feedback. Our paper’s reviews were
    split, with scores of 3, 2, 0, and -2. The meta reviewer notes "all four
    reviewers mentioned that the paper was clear, well-written, and
    well-executed - praising the online demo as well as the user interface." The
    meta reviewer states that their recommendation to reject was specifically
    based on two methodological concerns raised by R4. We believe both are
    serious, demonstrable material mistakes and would therefore like to appeal
    the decision. We present the evidence below:
  list:
    - >
      R4 experienced our tool crashing after repeatedly restricting feature
      ranges. We looked into this and have been unable to reproduce the bug.
      When we follow R4’s steps, our tool does not crash. <a
      href="#infeasible-video">Instead, it informs the user that no ILP solution
      exists.</a>
    - >
      R4 raised a serious concern that our tool would not generalize to other
      datasets/cases, and such differences could lead to inconsistent user
      experience. This is inaccurate, because:
    - >
      Our paper has described how our tool handles diverse datasets/cases, and
      linked to <a
      href="https://anonfacct.github.io/gam-coach/docs/gamcoach/gamcoach.html#get_model_data"
      target="_blank">detailed documentation</a> explaining how a practitioner
      can use our tool on different datasets.
    - >
      Our paper has also described how the interface design generalizes to any
      dataset with continuous and categorical variables, providing consistent
      user experience. As further evidence, we provide four additional working
      demonstrations in the supporting document to show such consistent user
      experience across commonly used datasets/cases. Thus, the user study
      performed on the LendingClub dataset is indeed significant in evaluating
      the tool’s generalizable usability.
  conclusion: >
    As R1, R2, R3, and the meta reviewer point out, the useful and usable user
    interface is our primary contribution, and this novel interface design meets
    critical needs. Therefore, we appeal to the FAccT committee to discard R4’s
    reviews and reconsider the final decision.

crash:
  title: 1. Tool does not crash when the ILP is infeasible
  review: >
    [R4] The tool does not currently handle cases where individuals might have
    no recourse (i.e., the ILP is infeasible). This is critical information that
    can be produced via an ILP approach. In this case, the simplest way to check
    is by forcing the tool to mine 1-feature changes and restricting the range
    of features that are proposed. Repeating this process several times leads to
    a setting where the ILP is infeasible and the tool crashes.
  response: >
    R4 experienced our tool crashing when the integer linear program (ILP) is
    infeasible. We have looked into this and have been unable to reproduce the
    reported bug. After forcing the tool to mine 1-feature changes and repeating
    the process to restrict the range of features, our tool does not crash.
    Instead, it displays error messages on the plan tab bar to inform users that
    no ILP solution exists (<a href="#infeasible-video">Video 1</a>).
  caption: >
    If no solution exists for an infeasible ILP, GAM Coach displays error
    messages on the corresponding plan tabs to inform users, and the tool is
    still usable. Screenshot is taken from the <a
    href="https://anonfacct.github.io/gam-coach" target="_blank">original demo
    page</a> (unchanged since paper submission).

development:
  title: 2 (a). Our paper has described how the tool handles different datasets/cases
  review: >
    [R4] The tool appears to be developed and evaluated with one model on one
    dataset. This is an issue given that there are serious differences between
    recourse and actionability across datasets and models (feasibility being one
    of them). In this case, there are two reasons to ask for a more
    comprehensive evaluation: (a) designing a tool that is robust and can be
    adapted to different cases;[...]
  response: >
    This comment is inaccurate. Our paper has described how GAM Coach handles
    different cases <span class="response-tag">[Page 15]</span> and provided a
    link <span class="response-tag">[Page 9]</span> to <a
      href="https://anonfacct.github.io/gam-coach/docs/gamcoach/gamcoach.html#get_model_data"
      target="_blank">detailed documentation</a>
    (unchanged since paper submission), which explains how a model developer can
    use our tool with GAMs trained on different datasets.
  quote: >
    [Page 15] Also, our open-source implementation handles special cases, such
    as features requiring integer values or using log transformations, and
    provides ML developers with simple APIs to integrate domain-specific
    descriptions when creating their own instance of GAM Coach.

experience:
  title: 2 (b). Our paper has described how the interface design provides a consistent user experience
  review: >
    [R4] […] (b) ensuring a consistent user experience across datasets
    (specifically as certain datasets might expose other kinds of effects).
  response: >
    This comment is inaccurate. As discussed in <span
    class="response-tag">Section 5.2</span>, with a flexible design of
    Continuous and Categorical Feature Cards, our tool provides a consistent
    user experience across different datasets. As further evidence, we provide
    four additional working demonstrations below to show such consistent user
    experience across commonly used datasets/cases in the algorithmic recourse
    literature (Table 1). Thus, the user study performed on the LendingClub
    dataset is indeed significant in evaluating the tool’s generalizable
    usability.
  caption: >
    We provide four working demonstrations in addition to the LendingClub
    dataset to show that GAM Coach provides a consistent user experience across
    commonly used datasets/cases in the algorithmic recourse literature.

evaluation:
  title: Evaluation needs to consider other datasets
  review: >
    [R4] In this case, there are two reasons to ask for a more comprehensive
    evaluation…
  response: >
    We explain our rationale for choosing LendingClub for our crowdworker user
    study on Page 9. Other commonly used datasets in algorithmic recourse
    literature are also lending related, but they are more dated and have fewer
    samples and features (Table 1). Compas is used to simulate a bail
    application scenario, but it is not reasonable for us to ask crowdworkers to
    pretend to be a bail applicant and ask for recourse.
  quote: >
    [Page 9] In this study, we chose a lending scenario because it is one domain
    where algorithmic recourse might be applied in practice, and there is no
    specialized knowledge needed to understand the setting. Therefore,
    crowdworkers would be relatively representative of the population who would
    use this tool.
